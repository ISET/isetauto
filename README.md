# ISETAUTO

We hope to learn how well machine-learning algorithms (mainly convolutional networks) generalize for different types of cameras.  The camera variations include lenses, CFAs and other design possibilities.

This repository includes the work we are doing to produce realistic driving scenes and that are acquired with a variety of camera designs.  We use the data from multiple camera designs to evaluate how well trained object detection networks generalize across camera designs.

Over time, the repository will contain the analyses from multiple papers on this topic.  This repository will depend on the isetcam, iset3d and isetcloud repositories.  In some instances, we may also depend on the RenderToolbox4 repository.

## Dependencies
1. ISETCAM
2. RenderToolbox4 (branch cloud)
3. RenderToolbox4-Mexximp
4. RenderToolbox4-mPBRT

## Directory structure

* Parameters - stores files describing different aspects of the camera, for 
example lens descriptions.
  * SceneArrangements - contains text files defining object positions within a particualr city block (```AssetPos_City_X_Placement_Y.json```) as well as camera positions and characteristics (```Cameras_City_X_Placement_Y.txt```). The camera position files are formatted according to the RenderToolbox4 conditions file specification.
* Code/Rendering - Matlab code that uses RTB4 to render images of cars in
urban environments


## Renderings

The current scene database consists of 20 different scenes. They were created by taking 4 different city blocks and randomly placing 5 different arrangements of objects within each block, producing 20 different geometries. To simulate image capture cameras were placed at 240 different, somewhat random, locations. Unfortunately some of the camera placements are 'unfortunate' because, for example, a camera ended up inside a building, and thus the simualted image does not contain any interesting content. Images were rendered using three different camera models and/or scenarios

For machine learning purposes all images which contained at elast one object of interest (car, pedestrian, bus, truck) were split into test and trainval datasets, containning about 800 and 2300 images respectively.


1. Pinhole camera model.
These are the base renderings generated with a pinhole camera. The data is placed in 
```
/sni/groups/wandel-test/projects/NN_Camera_Generalization/Renderings/MultiObject-Pinhole
```

2. Lens camera model.
This dataset contains images of the scene captured using the same camera as in 1. placed in the same spatial coordinates.
The difference is that this camera used a Double-Gaussian lens model rather than a pinhole. At each camera location the lens focus was changed from -20 to 20 diopters.

The data corresponding to each scene is stored in individual folders
```
/sni/groups/wandel-test/projects/NN_Camera_Generalization/Renderings/MultiObject-Lens-City-X-Placement-Y
```
where X=1,2,3,4 and Y=1,2,3,4,5.
Each scene file name is termiated with a sufix ```_def_BB.pbrt```, where BB denotes the amount of defocus in diopters.


3. Motion blur.
This collection of renderers can be used to simulate images with motion blur. Motion blurry images are generated by combining static images captured at discrete camera positions, where the camera traveling at a certain velocity would have been during the time the aperture was open.
This particualr collection of images is produced assuming that
<ul>
  <li> The camera moves along the lookAt (center of the image) direction. </li>
<li> The camera moves at a constant velocity of 20 m/s. </li>
  <li> The camera intergration time is 1/15s (or equivalently the camera operates at 15fps). </li>
<li> The total distance the camera covers in 1/15s is divided into 80 equal intervals, and a camera image for each of those locations is rendered (producing 80 images), as if the camera was completely static. </li>
<li> To produce a blurry image a certain number of the 80 frames should be added together. The number of frames to add will depend on the camera velocity and framerate. For example to simulate a 60fps camera traveling at 20m/s it would be sufficient to add only the first 20 frames from each sequence. </li>
  </ul>

The data corresponding to each scene is stored in individual folders:
```
/sni/groups/wandel-test/projects/NN_Camera_Generalization/Renderings/MultiObject-MotionBlur-City-X-Placement-Y
```
where X=1,2,3,4 and Y=1,2,3,4,5.
Each scene file name is terminated with a sufix ```_fps_15_vel_20_blr_ZZ.pbrt```, where ZZ denotes the index in the blur sequence, and ranges for 1 to 80.






